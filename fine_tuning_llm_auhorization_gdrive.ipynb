{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MoIhSJgeoHHi"
      },
      "outputs": [],
      "source": [
        "# Install the necessary libraries\n",
        "!pip install transformers datasets==2.4.0 evaluate sacrebleu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I try to ue the dataset from opus_books, and make translation model to translate from english to franch. In this code need the authrotize the goodle drive account because will save the model in the gdrive repositiry"
      ],
      "metadata": {
        "id": "48LxXZM43saL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaaYjtWfoXjC"
      },
      "outputs": [],
      "source": [
        "# import the library\n",
        "import os\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "books = load_dataset(\"opus_books\", \"en-fr\")\n",
        "\n",
        "# Split the Dataset\n",
        "# The dataset is splitted into train(80%) and test(20%)\n",
        "books = books[\"train\"].train_test_split(test_size=0.2)\n",
        "\n",
        "# I load the tokenizer from the T5 model (t5-small) to convert text into tokens that can be processed by the model.\n",
        "# \"google-t5/t5-small\", are widely used in various NLP applications because of its felxibility to handle tasks like translation\n",
        "checkpoint = \"google-t5/t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# Preprocessing the data\n",
        "source_lang = \"en\"\n",
        "target_lang = \"fr\"\n",
        "prefix = \"translate english to french: \"\n",
        "def preprocess_function(datas):\n",
        "    inputs = [prefix + data[source_lang] for data in datas[\"translation\"]]\n",
        "    targets = [data[target_lang] for data in datas[\"translation\"]]\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=64, truncation=True)\n",
        "    return model_inputs\n",
        "\n",
        "# Implement the preprocessing function to the dataset\n",
        "# To prepare data for training or inference.\n",
        "tokenized_books = books.map(preprocess_function, batched=True)\n",
        "\n",
        "# DataCollatorForSeq2Seq pads sequences dynamically in a batch for efficiency, ensuring inputs match the maximum length without padding the entire dataset.\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)\n",
        "\n",
        "# Evaluate the model using screblue. the purpose is to assess its performance on specific tasks, ensuring it generalizes well, meets quality standards, and identifies areas for improvement.\n",
        "metric = load_metric(\"sacrebleu\")\n",
        "\n",
        "# This function is to ensure that the data used for evaluation metrics are clean and consistent, improving the accuracy and reliability of the evaluation results\n",
        "# by removing unnecessary whitespace to prevent comparison errors between predictions and labels.\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "\n",
        "# Function to calculate the matrices\n",
        "# Using the prediction and labels to calculate SacreBLEU score\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    # If preds is a tuple, take the first element\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    # Decode predictions and labels\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    # Postprocess the decoded results\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "    # Calculate the metric\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "    # Calculate average prediction length\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    # Round results\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result\n",
        "\n",
        "# This step is to prepare the T5 model for further tasks, such as fine-tuning or inference, using the capabilities of the pre-trained weights.\n",
        "# This makes it easier to apply the model to specific applications without starting from scratch.\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
        "\n",
        "# Define the base output directory path in Google Drive\n",
        "drive.mount('/content/drive')\n",
        "base_output_dir = \"/content/drive/My Drive/llm_finetuning\"\n",
        "base_output_dir = \"/content/drive/My Drive/my_llm_model\"\n",
        "\n",
        "# Check if the directory already exists and rename if necessary\n",
        "output_dir = base_output_dir\n",
        "counter = 1\n",
        "while os.path.exists(output_dir):\n",
        "    output_dir = f\"{base_output_dir}_{counter}\"\n",
        "    counter += 1\n",
        "\n",
        "# The purpose of defining these hyperparameters is to configure the training process, optimizing for performance and efficiency while ensuring the model is trained effectively for the desired task.\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=64,\n",
        "    per_device_eval_batch_size=64,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=1,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    gradient_accumulation_steps=2,\n",
        ")\n",
        "\n",
        "\n",
        "# Preparing the Trainer\n",
        "# After setting up the hyperparameters, initialize Seq2SeqTrainer with the model, dataset, tokenizer, data collator, and metrics function defined earlier\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_books[\"train\"],\n",
        "    eval_dataset=tokenized_books[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Starting the Training\n",
        "trainer.train()\n",
        "\n",
        "# Saving the model and tokenizer after training\n",
        "model_dir = \"./my_finetuned_model\"\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(model_dir)\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save_pretrained(model_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tA7uNTJf-lgM"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "model_dir = './my_finetuned_model'\n",
        "\n",
        "# Initialize the translator for translating from English to French\n",
        "translator = pipeline(\"translation_en_to_fr\", model=model_dir)\n",
        "\n",
        "# Get the input from user\n",
        "text = input(\"Enter the text you want to translate to French: \")\n",
        "\n",
        "# The result of translation\n",
        "translated_result = translator(text)\n",
        "print(\"Translation Result:\", translated_result[0]['translation_text'])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}